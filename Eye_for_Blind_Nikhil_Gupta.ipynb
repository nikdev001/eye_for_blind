{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf5LdRf6QHls"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "import kagglehub\n",
    "flickr8k_path = kagglehub.dataset_download('nandakumardeva/flickr8k')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "026a72f0-2b9f-40a4-9473-409056155ea3",
    "_uuid": "96cdf1e5-ba87-4513-b7ca-061646028f05",
    "id": "c1CEYAPPQHlu"
   },
   "source": [
    "# EYE FOR BLIND - Capstone Project\n",
    "## upGrad PGDDS Course; Cohort 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W238iL8pQj0U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxnA7iIJQHlw"
   },
   "source": [
    "### By: Nandakumar Devadoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlzQaRt9QHlw"
   },
   "source": [
    "- This notebook is to be run on Kaggle as it requires a considerable amount of GPU power, and the environment on Kaggle allows for a smooth runing and functioning of the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BinVYx_ZQHlw"
   },
   "source": [
    "**Mandatory codeblock to facilitate running on Kaggle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "980060f5-6cd5-4ad8-96b6-22d53750ca8d",
    "_uuid": "887e5948-345d-4d17-af0a-41bcccf6c320",
    "collapsed": false,
    "id": "w3DVmTs5QHlw",
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDh7UJUNQHlx"
   },
   "source": [
    "### Importing  all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "263407df-2bad-4bcc-9e5b-87d760dac27a",
    "_uuid": "5fbd4bdf-bbba-4fcb-97b4-742682494258",
    "collapsed": false,
    "id": "KuUr8TN_QHlx",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageDraw\n",
    "import seaborn as sns\n",
    "from pickle import dump,load\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, Conv2D\n",
    "\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import Input, layers\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec882376-fc55-4fbc-8713-470f15cf6ace",
    "_uuid": "81534c8e-3579-4a82-b875-a8a108016d3c",
    "id": "KDfode0wQHlx"
   },
   "source": [
    "Let's read the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2574a34a-dc55-43a6-bc74-8f6089ad8fc7",
    "_uuid": "23117c7b-6538-453e-9c9e-7eadc44d2f4a",
    "id": "Me_z7yO-QHlx"
   },
   "source": [
    "## Data Understanding\n",
    "    1.Import the dataset and read image & captions into two seperate variables\n",
    "    2.Visualise both the images & text present in the dataset\n",
    "    3.Create word-to-index and index-to-word mappings.\n",
    "    4.Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "    5.Visualise the top 30 occuring words in the captions\n",
    "    6.Create a list which contains all the captions & path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIanzujhQHly"
   },
   "source": [
    "### 1.Import the dataset and read image & captions into two seperate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb97a44e-3ee1-4fdb-8036-402c4db88140",
    "_uuid": "f83b7e5d-7f24-49bc-985c-6283ad6f5334",
    "collapsed": false,
    "id": "_tAB8LfJQHly",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the image into a seperate variable\n",
    "\n",
    "images='/kaggle/input/flickr8k/Images'\n",
    "\n",
    "all_imgs = glob.glob(images + '/*.jpg',recursive=True)\n",
    "print(\"The total images present in the dataset: {}\".format(len(all_imgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zg3CyCx3QHly"
   },
   "source": [
    "### 2.Visualise both the images & text present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1514f625-03cc-4f6f-9514-7cca54080d61",
    "_uuid": "8f8b61e0-4dec-4148-a2c6-934224ab573a",
    "collapsed": false,
    "id": "vy16eCkSQHly",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Visualise both the images & text present in the dataset\n",
    "Image.open(all_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8194ff87-9599-43d8-98fc-c7f55c3a2495",
    "_uuid": "d11e6274-87c4-4f28-a4b6-43a30211d650",
    "collapsed": false,
    "id": "l75kUUESQHly",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_imgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e3VHlMRQHly"
   },
   "source": [
    "### 3.Create word-to-index and index-to-word mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "44835f23-0f81-4483-87fa-36a029169650",
    "_uuid": "8cddafb1-c499-4e6f-8ef2-6b10f90ff509",
    "collapsed": false,
    "id": "eHiNVdxcQHly",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Import the dataset and read the text file into a seperate variable\n",
    "\n",
    "def load_doc(filename):\n",
    "\n",
    "    text=open(filename).read()\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_img_ids_and_captions(text):\n",
    "    keys=[]\n",
    "    values=[]\n",
    "    key_paths=[]\n",
    "    text=text.splitlines()[1:]\n",
    "    for line in text:\n",
    "        com_idx=line.index(\",\")\n",
    "        im_id,im_cap=line[:com_idx],line[com_idx+1:]\n",
    "        keys.append(im_id)\n",
    "        values.append(im_cap)\n",
    "        key_paths.append(images+'/'+im_id)\n",
    "    return keys,values,key_paths\n",
    "\n",
    "text_file=\"/kaggle/input/flickr8k/captions.txt\"\n",
    "# text_file = 'D:/upGrad_Data_Science/Capstone - Eye for blind/Flickr8K/captions.txt'\n",
    "doc = load_doc(text_file)\n",
    "print(doc[:600], \"........\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGwjU-ZlQHly"
   },
   "source": [
    "### 4.Create a dataframe which summarizes the image, path & captions as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb9f528e-e1ee-40d6-9c9d-8774632fbd06",
    "_uuid": "bcac72de-840f-4018-81b4-dac2d8917704",
    "id": "IN86nLdQQHly"
   },
   "source": [
    "Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00b43e50-fc9b-471c-9b74-2294befdb8dc",
    "_uuid": "b727cb4e-c1ef-4173-b568-c100fda645e1",
    "collapsed": false,
    "id": "q2GVgP8-QHly",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_img_id,annotations,all_img_vector= get_img_ids_and_captions(doc)#store all the image id here\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5edfca6f-644b-4d4b-a7f9-a49f9361c66d",
    "_uuid": "a7a8f708-db77-41ed-a917-b23daed54fba",
    "collapsed": false,
    "id": "YEUmA3aHQHly",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "Image.open(all_img_vector[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e24ba3f9-13c7-41ce-98c4-ac81c8320ec8",
    "_uuid": "2445b33a-531c-41ce-b322-0acd00bcadca",
    "collapsed": false,
    "id": "NpEIANacQHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Create the vocabulary & the counter for the captions\n",
    "def voc_fetcher(frame,column):\n",
    "    out=[]\n",
    "    for i in frame[column]:\n",
    "        out+=i.split(\" \")\n",
    "    return out\n",
    "\n",
    "\n",
    "vocabulary=voc_fetcher(df,\"Captions\")\n",
    "val_count=Counter(vocabulary)\n",
    "val_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wstq9WYxQHlz"
   },
   "source": [
    "### 5.Visualise the top 30 occuring words in the captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9120a39a-5303-4d97-a189-50b9ff6927d8",
    "_uuid": "16c87f64-647b-4ebc-814f-d6c0bb13ad46",
    "collapsed": false,
    "id": "vDCtVO0iQHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Visualise the top 30 occuring words in the captions\n",
    "def get_top_words_based_on_cnt(words_dict,n_words):\n",
    "    n_words+=1\n",
    "    keys=list(words_dict.keys())\n",
    "    values=list(words_dict.values())\n",
    "    sorted_values=sorted(values,reverse=True)[:n_words]\n",
    "    sorted_keys=[]\n",
    "    for i in sorted_values:\n",
    "        if sorted_values.count(i)==1:\n",
    "            sorted_keys.append(keys[values.index(i)])\n",
    "        elif sorted_values.count(i)==2:\n",
    "            f_idx=values.index(i)\n",
    "            s_idx=values[f_idx+1:].index(i)\n",
    "            s_idx+=f_idx+1\n",
    "            a,b=keys[f_idx],keys[s_idx]\n",
    "            if a not in sorted_keys and b not in sorted_keys:\n",
    "                sorted_keys.append(a)\n",
    "                sorted_keys.append(b)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x=sorted_keys,y=sorted_values)\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.show()\n",
    "    #write your code here\n",
    "get_top_words_based_on_cnt(val_count,30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBF5gXCTQHlz"
   },
   "source": [
    "### 6.Create a list which contains all the captions & path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "87a7f69d-210b-4f5f-a210-f78c157ceec5",
    "_uuid": "afb304ae-c0ff-4cc0-a8a2-224e3a20c327",
    "collapsed": false,
    "id": "IAv3DuLjQHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Create a list which contains all the captions\n",
    "annotations=df.Captions.apply(lambda z:\"<start>\"+\" \"+z+\" \"+\"<end>\")\n",
    "\n",
    "#add the <start> & <end> token to all those captions as well\n",
    "\n",
    "\n",
    "#Create a list which contains all the path to the images\n",
    "all_img_path=df.Path.to_list()#write your code here\n",
    "\n",
    "print(\"Total captions present in the dataset: \"+ str(len(annotations)))\n",
    "print(\"Total images present in the dataset: \" + str(len(all_img_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8e39d5dd-873e-4c38-97f1-3f79a4c4cbab",
    "_uuid": "4a97fbf8-69d6-4453-800e-18399c53f4b7",
    "collapsed": false,
    "id": "RoiaSb7iQHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def caption_and_image_plotter(image_id,frame):\n",
    "    #get captions\n",
    "    caps=(\"\\n\"*2).join(frame[frame['ID']==image_id].Captions.to_list())\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    idx=df.ID.to_list().index(image_id)\n",
    "    im=Image.open(df.Path.iloc[idx])\n",
    "    w,h=im.size[0],im.size[-1]\n",
    "    ax.imshow(im)\n",
    "    ax.text(w+50,h,caps,fontsize=20,color='green')\n",
    "caption_and_image_plotter(df.ID.iloc[8049],df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "783e9b34-7990-462d-9795-27e86d6b096a",
    "_uuid": "55ae4d52-513d-47d9-b412-3b9978ae3251",
    "collapsed": false,
    "id": "tuoGrJi0QHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_caption_and_image_plotter_for_a_range(start,end,frame):\n",
    "    for i in range(start,end):\n",
    "        caption_and_image_plotter(frame.ID.drop_duplicates().iloc[i],frame)\n",
    "run_caption_and_image_plotter_for_a_range(0,5,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fa844680-53e7-4666-8c1d-82f32c2f9b4d",
    "_uuid": "66a47dd3-12db-4d44-b0fb-cf7c23319aed",
    "id": "WxL5gSiJQHlz"
   },
   "source": [
    "## Pre-Processing the captions\n",
    "1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters.\n",
    "This gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n",
    "\n",
    "2.Replace all other words with the unknown token \"UNK\" .\n",
    "\n",
    "3.Create word-to-index and index-to-word mappings.\n",
    "\n",
    "4.Pad all sequences to be the same length as the longest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c0186c68-fff8-40c0-aa14-450e8e1f8313",
    "_uuid": "99d36547-8e0b-48c6-82c3-2a68576f687d",
    "collapsed": false,
    "id": "NU7q-Ra8QHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "top_word_cnt = 5000\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_word_cnt,oov_token=\"<unk>\",filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "tokenizer.fit_on_texts(annotations)\n",
    "train_seqs = tokenizer.texts_to_sequences(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0820ad58-af1c-424b-b672-a97a2deb46a8",
    "_uuid": "f0f7cd81-4738-46b8-88e7-3e392540a3bf",
    "collapsed": false,
    "id": "fdbQIUVOQHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_seqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "657a3db2-7220-4d0e-9617-c3a7f39d36ad",
    "_uuid": "90249135-1c6a-4518-9a26-36b33b230a52",
    "collapsed": false,
    "id": "lE_X9AluQHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings.\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'\n",
    "train_seqs = tokenizer.texts_to_sequences(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3499e21e-1f5d-4aca-bfdf-93cebf5ee795",
    "_uuid": "ec4fff3f-ee58-4469-9ea9-f459a98b3317",
    "collapsed": false,
    "id": "Z3S_IWC9QHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "07ddcd88-92e4-453f-bee0-b7683875d09d",
    "_uuid": "df1218a1-4012-4fb7-a5e5-9712b5918a96",
    "collapsed": false,
    "id": "gtqjZ3bDQHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n",
    "get_top_words_based_on_cnt(tokenizer.word_counts,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3151dbb5-0f67-4d49-8ce4-8e058ec4eea9",
    "_uuid": "cbf76c5f-1fbb-411b-b1d6-640ec0b7f719",
    "collapsed": false,
    "id": "g7rP0ra7QHlz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def min_max_for_nested_array(nested_array):\n",
    "    array=[len(e) for e in nested_array]\n",
    "    return min(array),max(array)\n",
    "min_l,max_l=min_max_for_nested_array(train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fb9dd944-8ab6-4c3c-bd7b-ac43836e9f0f",
    "_uuid": "a30639e8-6631-43e0-961a-153398fc076b",
    "collapsed": false,
    "id": "HWPgvBMtQHl0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions ^ store it to a vairable\n",
    "\n",
    "cap_vector=tf.keras.preprocessing.sequence.pad_sequences(train_seqs,padding='post',maxlen=max_l)\n",
    "\n",
    "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e64fefd1-236e-4b3a-a539-bc59943d1786",
    "_uuid": "3767f840-2f96-4a7e-94e9-944f10a9eea8",
    "id": "UTnLj3aIQHl0"
   },
   "source": [
    "## Pre-processing the images\n",
    "\n",
    "1.Resize them into the shape of (299, 299)\n",
    "\n",
    "3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d4a796e9-c75a-432a-af2c-e68ac1b25796",
    "_uuid": "e965031f-676a-45d5-b9f9-cd9f7e3b1905",
    "collapsed": false,
    "id": "hEjQew0NQHl0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "def load_the_image(file_path):\n",
    "    my_img = tf.io.read_file(file_path)\n",
    "    my_img = tf.image.decode_jpeg(my_img, channels=3)\n",
    "    my_img = tf.image.resize(my_img, (299, 299))\n",
    "    my_img = tf.keras.applications.inception_v3.preprocess_input(my_img)\n",
    "    return my_img,file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a1726c00-471d-48a1-b45e-6a4d4ef7a3fc",
    "_uuid": "90c12366-71b3-47dc-91c1-ab19df974b88",
    "collapsed": false,
    "id": "iRvlDnEkQHl0",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(load_the_image(all_img_path[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "391d1266-1d68-45ef-9cbf-b86f5b1ca4f8",
    "_uuid": "edd2e51f-b76b-4030-85a9-132eae27aefb",
    "collapsed": false,
    "id": "c-QWw2twQHl4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "encode_train_set = sorted(set(all_img_vector))\n",
    "\n",
    "feature_dict = {}\n",
    "\n",
    "image_data_set = tf.data.Dataset.from_tensor_slices(encode_train_set)\n",
    "image_data_set = image_data_set.map(load_the_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f73c3d4a-d143-48e7-8fd1-85a09e3be69d",
    "_uuid": "9601d3cf-02f3-4004-95a0-1283f062f173",
    "collapsed": false,
    "id": "b_ua1FutQHl4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fd680411-773f-40d6-b19e-7e54d780bd4d",
    "_uuid": "a630b3e1-48c4-44a4-bca0-d8bf4b5baab4",
    "id": "x6NVfhxjQHl4"
   },
   "source": [
    "## Create the train & test data\n",
    "1.Combine both images & captions to create the train & test dataset using tf.data.Dataset API. Create the train-test spliit using 80-20 ratio & random state = 42\n",
    "\n",
    "2.Make sure you have done Shuffle and batch while building the dataset\n",
    "\n",
    "3.The shape of each image in the dataset after building should be (batch_size, 299, 299, 3)\n",
    "\n",
    "4.The shape of each caption in the dataset after building should be(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "60333e93-c554-4f1a-bf13-516ae5ad43fd",
    "_uuid": "51d8d0d3-6266-4734-9886-b1b21fd0b3cf",
    "collapsed": false,
    "id": "9sILQhDvQHl4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#write your code here\n",
    "image_train, image_test, caption_train, caption_test = train_test_split(all_img_vector,\n",
    "                                                                        cap_vector,\n",
    "                                                                        test_size=0.2,\n",
    "                                                                        random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3d52b3c5-38d2-457e-b43a-c9f811d796f8",
    "_uuid": "6fe8c521-5c81-40e6-81e4-91f15fc7f14a",
    "collapsed": false,
    "id": "u9Z3YaeFQHl4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training data for images: \" + str(len(image_train)))\n",
    "print(\"Testing data for images: \" + str(len(image_test)))\n",
    "print(\"Training data for Captions: \" + str(len(caption_train)))\n",
    "print(\"Testing data for Captions: \" + str(len(caption_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "028f2c06-fe06-46b7-b823-dae6b2d7e00b",
    "_uuid": "3e4d0ace-5495-4e98-bded-2daf7446d2d9",
    "collapsed": false,
    "id": "db_bMkl2QHl4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def map_function(image_name,capt):\n",
    "    image_tensor = feature_dict[image_name.decode('utf-8')]\n",
    "    return image_tensor,capt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e33442f9-689b-49dc-9a17-b0ef159359a2",
    "_uuid": "1259671a-c80c-4e01-bb49-d103645c2b47",
    "id": "QtsCJOTrQHl5"
   },
   "source": [
    "## Load the pretrained Imagenet weights of Inception net V3\n",
    "\n",
    "1.To save the memory(RAM) from getting exhausted, extract the features of thei mage using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n",
    "\n",
    "2.The shape of the output of this layer is 8x8x2048.\n",
    "\n",
    "3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7a75534f-b397-4a00-8ec1-9540a3897db1",
    "_uuid": "67c19e21-6aef-48b6-848f-f84389ac10f4",
    "collapsed": false,
    "id": "HJMghK4kQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "image_features_extract_model = keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d49b3a82-d091-4719-8397-8190a5a6babb",
    "_uuid": "d52d5eb3-2c31-4498-b45a-626629746021",
    "collapsed": false,
    "id": "MCLliDZiQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# write your code to extract features from each image in the dataset\n",
    "image_features_extract_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6ad18544-1bca-4ed4-ac9a-422f514384a9",
    "_uuid": "ec0724ec-f46e-488a-ad51-a25e5eefa000",
    "collapsed": false,
    "id": "GU9htSKJQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for image,path in tqdm(image_data_set):\n",
    "    batch_features = image_features_extract_model(image)\n",
    "    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "    for batch_f, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        feature_dict[path_of_feature] =  batch_f.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "28b47262-5a18-4e71-93c7-634513dd4819",
    "_uuid": "313be8a1-01df-4c46-8550-23b0d6afa347",
    "collapsed": false,
    "id": "Z0nDccr3QHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "batch_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da6d43e8-f3d2-4e75-8f73-912e7e6c8d6d",
    "_uuid": "16ac01f8-b601-4e91-b6b4-1291c74d4b93",
    "collapsed": false,
    "id": "6IVrF8-uQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "def generate_dataset(images_data, captions_data):\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images_data, captions_data))\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_function, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "113e911e-6e05-4d10-bb39-f7d6012e3760",
    "_uuid": "aebe712c-b341-4452-a1d1-c0699ef0539a",
    "collapsed": false,
    "id": "iVJu_o9zQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset=generate_dataset(image_train,caption_train)\n",
    "test_dataset=generate_dataset(image_test,caption_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b9bbf152-1dee-4576-9708-5833d22079a9",
    "_uuid": "a4b26cc6-5823-4e96-b467-be38b64f7a5c",
    "collapsed": false,
    "id": "0aVdqOwAQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n",
    "print(sample_cap_batch.shape) #(batch_size,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3500b448-e9a7-45e4-ae42-08ca32cee8ff",
    "_uuid": "6045caf7-9548-4a97-9366-41f971815bb8",
    "id": "T-5NuEv9QHl5"
   },
   "source": [
    "## Model Building\n",
    "1.Set the parameters\n",
    "\n",
    "2.Build the Encoder, Attention model & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0a0cdebb-6e19-4e88-b3f6-9076d10f9b80",
    "_uuid": "b311d6ac-93cc-4a15-81f9-18492947d3c4",
    "collapsed": false,
    "id": "UkJSfPUjQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 512\n",
    "vocab_size = 5001 #top 5,000 words +1\n",
    "train_num_steps = len(image_train) // BATCH_SIZE\n",
    "test_num_steps = len(image_test) // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "236a7658-68b5-4dc1-99c9-98889446ae7a",
    "_uuid": "f7610a33-188a-437b-8486-764ff33b7f6d",
    "id": "ucJqeN8AQHl5"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "85b8eb96-6ec6-49f2-a414-6c63d1c538ee",
    "_uuid": "ace8642a-8772-4f81-9f2b-d3d6e06e7a5a",
    "collapsed": false,
    "id": "ZMoRjJccQHl5",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = layers.Dense(embed_dim)\n",
    "        self.dropout = layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, features):\n",
    "        features = self.fc(features)\n",
    "        features = tf.nn.relu(features)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9b2b03fb-f91f-45f9-9051-f0d0c3ab9fee",
    "_uuid": "dc6b5217-0596-4da4-aa78-71c037a280f2",
    "collapsed": false,
    "id": "1bmmlJFQQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "encoder=Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e6326411-778b-40e5-88ed-71d24f44795f",
    "_uuid": "a8e4674f-30bf-4892-97ef-5092dc0d6e2f",
    "id": "QTpIPC00QHl6"
   },
   "source": [
    "### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6f37dff1-07fc-4493-9058-3c40844498c0",
    "_uuid": "60a5208b-4d1e-44b7-9d28-890b397b0aee",
    "collapsed": false,
    "id": "Qmm2n1cZQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "        self.units=units\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis =  tf.expand_dims(hidden, 1)\n",
    "        score = keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights =  keras.activations.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc89cc12-a11a-4c3a-b7c1-e710a0b3bfad",
    "_uuid": "a17ffc60-ead3-46d3-a3a6-f76b7ab7a0e7",
    "collapsed": false,
    "id": "2joj2a_FQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "\n",
    "        super(RNN_Decoder, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.fc1 = layers.Dense(self.units)\n",
    "        self.fc2 = layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = Attention_model(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "        x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2cc0ff9d-1a12-4699-9c61-bb65f45d1dc0",
    "_uuid": "aaee0e1b-8ce7-4b0e-bc46-386c0baac2d7",
    "id": "x--w8kBAQHl6"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f1cbcea7-17a1-47a7-bf0f-88bd44d27b72",
    "_uuid": "7979df2c-abe8-4036-b6bc-aceb77c8b17e",
    "collapsed": false,
    "id": "ADFW78WqQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units=units\n",
    "        self.attention = Attention_model(self.units)\n",
    "        self.embed = layers.Embedding(vocab_size, embed_dim,mask_zero=False)\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        self.d1 = layers.Dense(self.units)\n",
    "        self.d2 = layers.Dense(vocab_size)\n",
    "        self.dropout = Dropout(0.5)\n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden)\n",
    "        embed = self.dropout(self.embed(x))\n",
    "        mask = self.embed.compute_mask(x)\n",
    "        embed =  tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)\n",
    "        output,state = self.gru(embed,mask=mask)\n",
    "        output = self.d1(output)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        output = self.d2(output)\n",
    "\n",
    "        return output,state, attention_weights\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "073154a5-76df-4c1e-b076-9686f2e757c3",
    "_uuid": "8bedae60-d138-4323-8406-7ad9b76fb26e",
    "collapsed": false,
    "id": "C9gkEJ2cQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "decoder=Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "04fefb04-baf8-4487-b030-5eb0e681a4ff",
    "_uuid": "db1860f0-16aa-4d98-a389-7c81fb56eb0e",
    "collapsed": false,
    "id": "FLFlMamCQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features=encoder(sample_img_batch)\n",
    "\n",
    "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
    "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
    "\n",
    "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
    "print('Feature shape from Encoder: {}'.format(features.shape))\n",
    "print('Predcitions shape from Decoder: {}'.format(predictions.shape))\n",
    "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "28dc015f-6487-4c04-a4bc-f1a7c9fba128",
    "_uuid": "e23b37f7-ee8e-486f-9f1a-2c7cecd0c06d",
    "id": "TkNDaEPIQHl6"
   },
   "source": [
    "## Model training & optimization\n",
    "1.Set the optimizer & loss object\n",
    "\n",
    "2.Create your checkpoint path\n",
    "\n",
    "3.Create your training & testing step functions\n",
    "\n",
    "4.Create your loss function for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f182782b-bb60-4720-ac1e-33843adf52cb",
    "_uuid": "0efeee98-012b-47f5-bee2-0449c7cf4bac",
    "collapsed": false,
    "id": "bVEFPmutQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09fbd55e-7510-49e1-b540-0925dd685be1",
    "_uuid": "f19ba8fa-2f78-4660-b570-418885bf83e0",
    "collapsed": false,
    "id": "tKcbLW3uQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "372751c1-8ed8-4756-b50b-527930873638",
    "_uuid": "6824a3f2-5c2b-4f22-a368-a927c4705f5d",
    "collapsed": false,
    "id": "b7YJELqsQHl6",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"flickr8k/File4/\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "                                          checkpoint_path,\n",
    "                                          max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6a7525d6-2039-4b4a-979f-98d3a6ec8384",
    "_uuid": "146b2798-2ce3-4d13-b4ea-8a90c412e145",
    "collapsed": false,
    "id": "N6iAW8d0QHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e9c81f5-05e0-427a-b160-187a33d5b030",
    "_uuid": "5be06fff-68fa-4bf4-bfe1-3e62bb300348",
    "collapsed": false,
    "id": "XlcJor4WQHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "        avg_loss = (loss/int(target.shape[1]))\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "        return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "647d18ab-87cc-486e-8b08-2a0d7d5dd38d",
    "_uuid": "ea2a07ac-0acc-47df-bc47-7f372539b0be",
    "collapsed": false,
    "id": "gv0FTyqlQHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "        avg_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d7530a86-79f1-4c4b-b10b-e245ec8fc30f",
    "_uuid": "a2b437ca-a7f8-4ba4-b958-c3d0d459f645",
    "collapsed": false,
    "id": "4-EVf1MyQHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def test_loss_cal(test_dataset):\n",
    "    total_loss = 0\n",
    "    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n",
    "        batch_loss, t_loss = test_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "    avg_test_loss=total_loss/test_num_steps\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ae848309-92ca-4bed-9012-d24b7848bfca",
    "_uuid": "d9140787-55e6-46d2-b5b1-ffb25a25148e",
    "collapsed": false,
    "id": "EIqVHUzMQHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "test_loss_plot = []\n",
    "EPOCHS = 15\n",
    "\n",
    "best_test_loss=100\n",
    "for epoch in tqdm(range(0, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_train_loss=total_loss / train_num_steps\n",
    "\n",
    "    loss_plot.append(avg_train_loss)\n",
    "    test_loss = test_loss_cal(test_dataset)\n",
    "    test_loss_plot.append(test_loss)\n",
    "\n",
    "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    if test_loss < best_test_loss:\n",
    "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
    "        best_test_loss = test_loss\n",
    "        ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6078fd78-bbd6-48b1-a54e-6daa9b360052",
    "_uuid": "72df27cc-8906-4952-a48d-1c0376afbaf0",
    "collapsed": false,
    "id": "04YnFXufQHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.plot(test_loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "32aeb569-b62a-4341-8d8e-7259ed9e8491",
    "_uuid": "81c56dc7-9401-4198-ba64-e568a96d0f4e",
    "id": "NiKU-XePQHl7"
   },
   "source": [
    "## Model Evaluation\n",
    "1.Define your evaluation function using greedy search\n",
    "\n",
    "2.Define your evaluation function using beam search ( optional)\n",
    "\n",
    "3.Test it on a sample data using BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a8f5683-5927-4793-ae5d-8335068070f1",
    "_uuid": "d83f762a-cf0b-4d71-96be-9c7318b3692f",
    "id": "dhcvqMWXQHl7"
   },
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "50f96d7f-c5aa-4e9b-b309-849e6f2511a4",
    "_uuid": "fc380b03-88ff-4996-a83c-0cea5d6e2c08",
    "collapsed": false,
    "id": "vlYX8PqBQHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    max_length=max_l\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_the_image(image)[0], 0) #process the input image to desired format before extracting features\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot,predictions\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot,predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24a837d5-2951-4fa2-9eee-e2c35de0d6fc",
    "_uuid": "c751ef0e-b88a-4dec-8e1d-c796f6565b92",
    "id": "y8Nnt-xIQHl7"
   },
   "source": [
    "### Beam Search(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a229fc8b-7f22-4ccf-8553-393253742031",
    "_uuid": "21649327-0efb-42fc-9550-4a9e5288aa6e",
    "collapsed": false,
    "id": "IbIRVy-WQHl7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def beam_evaluate(image, beam_index = 3):\n",
    "    max_length=max_l\n",
    "    start = [tokenizer.word_index['<start>']]\n",
    "    result = [[start, 0.0]]\n",
    "\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.init_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_the_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    while len(result[0][0]) < max_length:\n",
    "        i=0\n",
    "        temp = []\n",
    "        for s in result:\n",
    "            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "            i=i+1\n",
    "            word_preds = np.argsort(predictions[0])[-beam_index:]\n",
    "\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = s[0][:], s[1]\n",
    "                next_cap.append(w)\n",
    "\n",
    "                prob += np.log(predictions[0][w])\n",
    "\n",
    "                temp.append([next_cap, prob])\n",
    "        result = temp\n",
    "        result = sorted(result, reverse=False, key=lambda l: l[1])\n",
    "        result = result[-beam_index:]\n",
    "\n",
    "\n",
    "        predicted_id = result[-1]\n",
    "        pred_list = predicted_id[0]\n",
    "\n",
    "        prd_id = pred_list[-1]\n",
    "        if(prd_id!=3):\n",
    "            dec_input = tf.expand_dims([prd_id], 0)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "    result2 = result[-1][0]\n",
    "\n",
    "    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n",
    "    final_caption = []\n",
    "    for i in intermediate_caption:\n",
    "        if i != '<end>':\n",
    "            final_caption.append(i)\n",
    "\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "db96fe24-ea9b-4af0-9fd1-f4e99f28bfa0",
    "_uuid": "61e90459-571a-4e7d-82ab-2655d2e53da6",
    "collapsed": false,
    "id": "KCvJJ8e1QHl8",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_attmap(caption, weights, image):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    temp_img = np.array(Image.open(image))\n",
    "\n",
    "    len_cap = len(caption)\n",
    "    for cap in range(len_cap):\n",
    "        weights_img = np.reshape(weights[cap], (8,8))\n",
    "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
    "\n",
    "        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n",
    "        ax.set_title(caption[cap], fontsize=15)\n",
    "\n",
    "        img=ax.imshow(temp_img)\n",
    "\n",
    "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
    "        ax.axis('off')\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "75a7ade5-97d9-4f5c-846b-b833f1037eb1",
    "_uuid": "187d9603-adf6-47b8-9bc2-27bc0522c004",
    "collapsed": false,
    "id": "AEXM7dJJQHl8",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0f002262-c5a7-4564-9dcd-f24bae907263",
    "_uuid": "50a0d2e4-a9cd-48f9-8683-80241cc0f6c9",
    "collapsed": false,
    "id": "-Frfyez2QHml",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def filt_text(text):\n",
    "    filt=['<start>','<unk>','<end>']\n",
    "    temp= text.split()\n",
    "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
    "    text=' '.join(temp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bebffb92-7ef2-40d5-a706-de858b0b5d5c",
    "_uuid": "2406ef80-b8bf-4e08-908c-b8283a8d432e",
    "collapsed": false,
    "id": "cCMxbTGVQHml",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4813c705-a8fe-4fec-ac63-d7244d520bac",
    "_uuid": "3167c981-4b6c-4bdb-b31a-cff35df7eb36",
    "collapsed": false,
    "id": "B-igHTVsQHmm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "features_shape = batch_f.shape[1]\n",
    "attention_features_shape = batch_f.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "10551407-04d9-423e-994f-8549858ec82c",
    "_uuid": "48e8ec90-b5f7-4c14-8f34-0cb58db461da",
    "collapsed": false,
    "id": "VZmjIEkqQHmm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(image_test))\n",
    "print(rid)\n",
    "test_image = image_test[rid]\n",
    "\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in caption_test[rid] if i not in [0]])\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "\n",
    "\n",
    "real_caption=filt_text(real_caption)\n",
    "\n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(f\"BELU score: {score*100}\")\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', pred_caption)\n",
    "plot_attmap(result, attention_plot, test_image)\n",
    "\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bddac91e-1c1d-4bd2-84de-e9d4989c7e98",
    "_uuid": "0be37e59-5ce1-4eae-b8ff-007dcb3e148d",
    "collapsed": false,
    "id": "D2mspMaFQHmm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "captions=beam_evaluate(test_image)\n",
    "print(pred_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2723363d-6ac3-4601-9ebc-486f98750596",
    "_uuid": "b248a991-9335-4f89-9fb8-4d83f5093538",
    "collapsed": false,
    "id": "vIMffbaFQHmm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Converting Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gM5fFiX2QHmm"
   },
   "outputs": [],
   "source": [
    "! pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8G4ULghiQHmn"
   },
   "outputs": [],
   "source": [
    "# Import the required module for text to speech conversion\n",
    "from gtts import gTTS\n",
    "\n",
    "# Language in which you want to convert\n",
    "language = 'en'\n",
    "\n",
    "# Passing the text and language to the engine,\n",
    "myobj = gTTS(text=pred_caption, lang=language, slow=False)\n",
    "\n",
    "# Saving the converted audio in a mp3 file named\n",
    "myobj.save(\"Predicted_text.mp3\")\n",
    "\n",
    "# Playing the converted file\n",
    "os.system(\"./Predicted_text.mp3\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Eye for Blind - Aravind Molugu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
